<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>第 20 章 自然语言处理</title><link rel="stylesheet" type="text/css" href="../../docbook.css" /><meta name="generator" content="DocBook XSL Stylesheets Vsnapshot" /><meta name="keywords" content="php,pear,pecl,phar, python, , " /><link rel="home" href="../../index.html" title="Netkiller Python 手札" /><link rel="up" href="../index.html" title="部分 III. 人工智能 AI" /><link rel="prev" href="../ch19s05.html" title="19.5. 摄像头识别人脸" /><link rel="next" href="wordcloud.html" title="20.2. wordcloud" /></head><body><a xmlns="" href="//www.netkiller.cn/">Home</a> |
		<a xmlns="" href="//netkiller.github.io/">简体中文</a> |
	    <a xmlns="" href="http://netkiller.sourceforge.net/">繁体中文</a> |
	    <a xmlns="" href="/journal/index.html">杂文</a> |
	    <a xmlns="" href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a> |
   	    <a xmlns="" href="https://edu.51cto.com/lecturer/1703915.html">视频教程</a> |
	    <a xmlns="" href="https://space.bilibili.com/486347986/">bilibili</a> |
	    <a xmlns="" href="https://github.com/netkiller">Github</a> |
	    <a xmlns="" href="http://my.oschina.net/neochen/">OSChina 博客</a> |
	    <a xmlns="" href="https://cloud.tencent.com/developer/column/2078">云社区</a> |
	    <a xmlns="" href="https://yq.aliyun.com/u/netkiller/">云栖社区</a> |
	    <a xmlns="" href="https://www.facebook.com/bg7nyt">Facebook</a> |
	    <a xmlns="" href="http://cn.linkedin.com/in/netkiller/">Linkedin</a> |
	    <a xmlns="" href="//www.netkiller.cn/home/donations.html">打赏(Donations)</a> |
	    <a xmlns="" href="//www.netkiller.cn/home/about.html">About</a><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">第 20 章 自然语言处理</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="../ch19s05.html">上一页</a> </td><th width="60%" align="center">部分 III. 人工智能 AI</th><td width="20%" align="right"> <a accesskey="n" href="wordcloud.html">下一页</a></td></tr></table><hr /></div><table xmlns=""><tr><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=watch&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=fork&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;type=follow&amp;count=true&amp;size=large" height="30" width="240" frameborder="0" scrolling="0" style="width:240px; height: 30px;" allowTransparency="true"></iframe></td><td></td><td><a href="https://zhuanlan.zhihu.com/netkiller"><img src="/images/logo/zhihu-card-default.svg" height="25" /></a></td><td valign="middle"><a href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a> ｜ <a href="https://www.zhihu.com/club/1241768772601950208">多维度架构</a></td><td> | </td><td>微信号 netkiller-ebook  </td><td> | </td><td>51CTO：<a href="https://edu.51cto.com/lecturer/1703915.html">视频教程</a></td></tr></table><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="index"></a>第 20 章 自然语言处理</h2></div></div></div><div class="toc"><p><strong>目录</strong></p><dl class="toc"><dt><span class="section"><a href="index.html#jieba">20.1. 结巴中文分词</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#idm74018108368">20.1.1. 分词演示</a></span></dt><dt><span class="section"><a href="index.html#idm74018106720">20.1.2. 返回 generator</a></span></dt><dt><span class="section"><a href="index.html#idm74018103840">20.1.3. 返回 list</a></span></dt><dt><span class="section"><a href="index.html#idm74018102352">20.1.4. 精准模式与全模式比较</a></span></dt><dt><span class="section"><a href="index.html#idm74018099184">20.1.5. 精准模式与搜索引擎模式比较</a></span></dt><dt><span class="section"><a href="index.html#idm74018096256">20.1.6. 词性标注</a></span></dt><dt><span class="section"><a href="index.html#idm74018092384">20.1.7. 词典管理</a></span></dt><dt><span class="section"><a href="index.html#idm74018089632">20.1.8. analyse</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#idm74018088912">20.1.8.1. 提取标签</a></span></dt><dt><span class="section"><a href="index.html#idm74018086608">20.1.8.2. 基于 TextRank 算法的关键词抽取</a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="wordcloud.html">20.2. wordcloud</a></span></dt><dd><dl><dt><span class="section"><a href="wordcloud.html#wordcloud_cli">20.2.1. wordcloud_cli</a></span></dt><dt><span class="section"><a href="wordcloud.html#idm74018079808">20.2.2. WordCloud 对象配置参数</a></span></dt><dt><span class="section"><a href="wordcloud.html#idm74018077056">20.2.3. 与分词共用</a></span></dt><dt><span class="section"><a href="wordcloud.html#idm74018075904">20.2.4. 遮罩图</a></span></dt></dl></dd></dl></div>
	
	<p>自然语言处理（Natural Language Processing）</p>
	<p>中文分词(Chinese Word Segmentation)，英文是由单词组成，并使用空格来分开每个单词，而中文以字为单位，由字组成词，字于词的含有可能完全不同，因此，中文分词NPL相比英文分词要复杂的多。</p>
	<div class="itemizedlist"><p class="title"><strong>中文分词技术主要使用场景有哪些：</strong></p><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">搜索优化，关键词提取</li><li class="listitem">语义分析</li><li class="listitem">非结构化文本媒体内容，如社交信息</li><li class="listitem">文本聚类，根据内容生成自动分类</li><li class="listitem">文章自动标签</li><li class="listitem">情感分析</li><li class="listitem">词性提取</li></ul></div>
	
<div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="jieba"></a>20.1. 结巴中文分词</h2></div></div></div>
	
	<p>
		<a class="ulink" href="https://github.com/fxsjy/jieba" target="_top">https://github.com/fxsjy/jieba</a>
	</p>
	<p>安装</p>
	<pre class="screen">
		
pip install jieba
pip install paddlepaddle
		
	</pre>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm74018108368"></a>20.1.1. 分词演示</h3></div></div></div>
		
		<pre class="programlisting">
		
# encoding=utf-8
import jieba
import paddle
paddle.enable_static()
jieba.enable_paddle()  # 启动paddle模式。 
strs = ["我来到北京清华大学", "乒乓球拍卖完了", "中国科学技术大学"]
for str in strs:
    seg_list = jieba.cut(str, use_paddle=True)  # 使用paddle模式
    print("Paddle Mode: " + '/'.join(list(seg_list)))

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print(", ".join(seg_list))
		
		</pre>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm74018106720"></a>20.1.2. 返回 generator</h3></div></div></div>
		
		<p>默认 cut 返回 generator</p>
		<pre class="programlisting">
		
# encoding=utf-8
import jieba
import paddle

segs = jieba.cut("转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。")
print(type(segs))
print(", ".join(segs))		
		
		</pre>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm74018103840"></a>20.1.3. 返回 list</h3></div></div></div>
		
		<pre class="programlisting">
		
# encoding=utf-8
import jieba
import paddle

segs = jieba.lcut("转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。")
print(type(segs))
print(", ".join(segs))		
		
		</pre>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm74018102352"></a>20.1.4. 精准模式与全模式比较</h3></div></div></div>
		
		<pre class="programlisting">
		
# encoding=utf-8
import jieba
import paddle
text = "转载请与作者联系，同时请务必标明文章原始出处和作者信息及本声明。"
segs = jieba.cut(text)  # 精准模式
print(", ".join(segs))
print("=" * 50)
segs = jieba.cut(text, cut_all=True)  # 全模式
print(", ".join(segs))
		
		</pre>
		<p>输出结果</p>
		<pre class="screen">
		
neo@MacBook-Pro-Neo ~/workspace/python/jieba % python3.9 /Users/neo/workspace/python/jieba/lcut.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.787 seconds.
Prefix dict has been built successfully.
转载, 请, 与, 作者, 联系, ，, 同时, 请, 务必, 标明, 文章, 原始, 出处, 和, 作者, 信息, 及本, 声明, 。
==================================================
转载, 请, 与, 作者, 联系, ，, 同时, 请, 务必, 标明, 明文, 文章, 原始, 出处, 和, 作者, 信息, 及, 本, 声明, 。		
		
		</pre>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm74018099184"></a>20.1.5. 精准模式与搜索引擎模式比较</h3></div></div></div>
		
		<pre class="programlisting">
		
# encoding=utf-8
import jieba
import paddle
text = "小明硕士毕业于中国科学院计算所，后在日本京都大学深造"
segs = jieba.cut(text)  # 精准模式
print(", ".join(segs))
print("=" * 50)
searchs = jieba.cut_for_search(text)  # 搜索引擎模式
print(", ".join(searchs))
		
		</pre>
		<p>输出结果</p>
		<pre class="screen">
		
neo@MacBook-Pro-Neo ~/workspace/python/jieba % python3.9 /Users/neo/workspace/python/jieba/search.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.807 seconds.
Prefix dict has been built successfully.
小明, 硕士, 毕业, 于, 中国科学院, 计算所, ，, 后, 在, 日本京都大学, 深造
==================================================
小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造	
		
		</pre>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm74018096256"></a>20.1.6. 词性标注</h3></div></div></div>
		
		<pre class="screen">
		
标签	含义		标签	含义		标签	含义		标签	含义
n	普通名词	f	方位名词	s	处所名词	t	时间
nr	人名		ns	地名		nt	机构名	nw	作品名
nz	其他专名	v	普通动词	vd	动副词	vn	名动词
a	形容词	ad	副形词	an	名形词	d	副词
m	数量词	q	量词		r	代词		p	介词
c	连词		u	助词		xc	其他虚词	w	标点符号
PER	人名		LOC	地名		ORG	机构名	TIME	时间
		
		
		</pre>
		<p></p>
		<pre class="programlisting">
		
import jieba
import jieba.posseg as pseg
import paddle
words = pseg.cut("我爱北京天安门")  # jieba默认模式
for word, flag in words:
    print('%s %s' % (word, flag))

print("="*40)

paddle.enable_static()
jieba.enable_paddle()  # 启动paddle模式。
words = pseg.cut("我爱北京天安门", use_paddle=True)  # paddle模式
for word, flag in words:
    print('%s %s' % (word, flag))
		
		
		</pre>
		<p></p>
		<pre class="screen">
		
neo@MacBook-Pro-Neo ~/workspace/python % python3.9 /Users/neo/workspace/python/jieba/seg.py
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/2f/jfnljdpn1t1dj_f61z2s8bwm0000gn/T/jieba.cache
Loading model cost 0.753 seconds.
Prefix dict has been built successfully.
我 r
爱 v
北京 ns
天安门 ns
========================================
Paddle enabled successfully......
我 r
爱 v
北京 LOC
天安门 LOC		
		
		</pre>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm74018092384"></a>20.1.7. 词典管理</h3></div></div></div>
		
		<pre class="programlisting">
		
import jieba

text = "他来到了网易杭研大厦"

words = jieba.cut(text, HMM=False)
print(", ".join(words))
print("-" * 50)
jieba.add_word('杭研大厦')  # 将 “杭研大厦” 添加到词典
jieba.add_word('来到了')  # 将 “来到了” 添加到词典
words = jieba.cut(text, HMM=False)
print(", ".join(words))
print("-" * 50)
jieba.del_word('深圳')  # 将 “深圳” 从词典中删除
words = jieba.cut("我爱深圳", HMM=False)
print(", ".join(words))
		
		</pre>
		<p>自定义词典</p>
		<pre class="programlisting">
		
import jieba

jieba.load_userdict('dict.txt') # 载入用户词典
seg_list = jieba.cut("他来到了网易杭研大厦", HMM=False)  
print(", ".join(seg_list))

		
		</pre>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idm74018089632"></a>20.1.8. analyse</h3></div></div></div>
		

		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idm74018088912"></a>20.1.8.1. 提取标签</h4></div></div></div>
			
			<pre class="screen">
		
方法参数：
jieba.analyse.extract_tags(sentence, topK=5, withWeight=True, allowPOS=())
参数说明：
sentence 需要提取的字符串，必须是str类型，不能是list
topK 提取前多少个关键字
withWeight 是否返回每个关键词的权重
allowPOS是允许的提取的词性，默认为allowPOS=‘ns’, ‘n’, ‘vn’, ‘v’，提取地名、名词、动名词、动词	
		
			</pre>
			<pre class="programlisting">
			
file = open('article.txt', 'r', encoding='utf-8')
contents = file.read()
print(jieba.analyse.extract_tags(sentence=contents, topK=20, allowPOS=('ns', 'n')))			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idm74018086608"></a>20.1.8.2. 基于 TextRank 算法的关键词抽取</h4></div></div></div>
			
			<pre class="programlisting">
			
import jieba.analyse
import jieba
import os

os.chdir('jieba')
file = open('article.txt', 'r', encoding='utf-8')
contents = file.read()
print(jieba.analyse.textrank(sentence=contents, topK=20, allowPOS=('ns', 'n')))			
			
			</pre>
		</div>
	</div>
</div>

	

</div>

        网站：<a xmlns="" href="//www.netkiller.cn/">http://www.netkiller.cn/</a> |
	    知乎：<a xmlns="" href="https://www.zhihu.com/people/netkiller">netkiller</a> |
	    51CTO：<a xmlns="" href="https://edu.51cto.com/lecturer/1703915.html">视频教程</a> |
	    Bilibili：<a xmlns="" href="https://space.bilibili.com/486347986/">netkiller</a> |
	    Github：<a xmlns="" href="https://github.com/netkiller">netkiller</a><div xmlns="" id="disqus_thread"></div><script xmlns="">

var disqus_config = function () {
this.page.url = "http://www.netkiller.cn";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'netkiller'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//netkiller.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script><noscript xmlns="">Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><br xmlns="" /><script xmlns="" type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=r5HG&amp;d=9mi5r_kkDC8uxG8HuY3p4-2qgeeVypAK9vMD-2P6BYM"></script><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="../ch19s05.html">上一页</a> </td><td width="20%" align="center"><a accesskey="u" href="../index.html">上一级</a></td><td width="40%" align="right"> <a accesskey="n" href="wordcloud.html">下一页</a></td></tr><tr><td width="40%" align="left" valign="top">19.5. 摄像头识别人脸 </td><td width="20%" align="center"><a accesskey="h" href="../../index.html">起始页</a></td><td width="40%" align="right" valign="top"> 20.2. wordcloud</td></tr></table></div><script xmlns="">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-11694057-1', 'auto');
  ga('send', 'pageview');

</script><script xmlns="" async="async">
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?93967759a51cda79e49bf4e34d0b0f2c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script xmlns="" async="async">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script></body></html>